{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import foolbox\n",
    "import torch\n",
    "import random\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import viz_2d as viz\n",
    "import acd\n",
    "import dset\n",
    "\n",
    "# %%\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "sys.path.append('/run/media/xiangyu/Data/Projects/XAI/BHEM')\n",
    "from model import Cnn, getClassifier\n",
    "from dataset import handwriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_type = 'mnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dataset.mnist.handwriting'> <class 'numpy.ndarray'> <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "[5 0 4 ... 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "mnist = handwriting('mnist_784', normalize=True)\n",
    "print(type(mnist), type(mnist.X), type(mnist.y), type(mnist.XCnn))\n",
    "print(mnist.y)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.imshow(mnist.XCnn[0].reshape(28, 28), cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), (70000,), (70000, 1, 28, 28))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.X.shape, mnist.y.shape, mnist.XCnn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cnn(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1600, out_features=100, bias=True)\n",
       "  (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
       "  (fc1_drop): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Cnn()\n",
    "checkpoint = torch.load('../../MINST.pkl', map_location=torch.device('cpu'))\n",
    "model.load_state_dict(checkpoint)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 28, 28]), torch.Size([1, 10]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xsample = torch.tensor(mnist.XCnn[0:1]).to(device)\n",
    "X_preds = model(Xsample)\n",
    "Xsample.shape, X_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn = getClassifier(Cnn, device, f_params='../../MINST.pkl')\n",
    "# X_preds = cnn.predict_proba(Xsample)\n",
    "# Xsample.shape, X_preds.shape\n",
    "# X_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AcdExp:\n",
    "    def __init__(self, image = None, sweep_dim=1):\n",
    "       # image: tensor(1,1,28,28)\n",
    "       self.image = image\n",
    "       self.sweep_dim = sweep_dim\n",
    "\n",
    "    def get_diff_scores(self, im_torch, im_orig, label_num, model, preds, sweep_dim):\n",
    "        '''Computes different attribution scores\n",
    "        '''\n",
    "        scores = []\n",
    "\n",
    "        # cd\n",
    "        # method = 'cd'\n",
    "        # tiles = acd.tiling_2d.gen_tiles(im_orig, fill=0, method=method, sweep_dim=sweep_dim)\n",
    "        # scores_cd = acd.get_scores_2d(model, method=method, ims=tiles, \n",
    "        #                             im_torch=im_torch, model_type=model_type, device=device)\n",
    "        # scores.append(scores_cd)\n",
    "\n",
    "        for method in ['occlusion', 'build_up']: # 'build_up'\n",
    "            tiles_break = acd.tiling_2d.gen_tiles(im_orig, fill=0, method=method, sweep_dim=sweep_dim)\n",
    "            preds_break = acd.get_scores_2d(model, method=method, ims=tiles_break, \n",
    "                                                im_torch=im_torch, pred_ims=dset.pred_ims)\n",
    "            if method == 'occlusion':\n",
    "                preds_break += preds\n",
    "            scores.append(np.copy(preds_break))\n",
    "        \n",
    "        # get integrated gradients scores\n",
    "        scores.append(acd.ig_scores_2d(model, im_torch, num_classes=10, \n",
    "                                            im_size=28, sweep_dim=sweep_dim, ind=[label_num], device=device))\n",
    "        return scores\n",
    "\n",
    "    def get_explanation(self, model, label):\n",
    "        # model is cnn object\n",
    "        # self.image is (1,1,28,28) tensor\n",
    "        X_orig = self.image.view(self.image.shape[-2], self.image.shape[-1]).cpu().numpy()\n",
    "        # pred.size = (10,)\n",
    "        preds = model(self.image).flatten().cpu().detach().numpy()\n",
    "        scores = self.get_diff_scores(self.image, X_orig, label, model, preds, self.sweep_dim)\n",
    "        return scores[0]\n",
    "\n",
    "            # # plot raw image\n",
    "            # num_rows = len(im_nums)\n",
    "            # num_cols = len(scores) + 1\n",
    "            # plt.subplot(num_rows, num_cols, 1 + x * num_cols)\n",
    "            # plt.imshow(im_orig, cmap='gray')\n",
    "            # plt.gca().xaxis.set_visible(False)\n",
    "            # plt.yticks([])\n",
    "            # if x == 0:\n",
    "            #     plt.title('Image', fontsize=16)\n",
    "\n",
    "\n",
    "            # # plot scores\n",
    "            # vmax = max([np.max(scores[i]) for i in range(len(scores))])\n",
    "            # vmin = min([np.min(scores[i]) for i in range(len(scores))])\n",
    "            # vabs = max(abs(vmax), abs(vmin))\n",
    "            # for i, tit in enumerate(['CD', 'Occlusion', 'Build-Up', 'IG']):\n",
    "            #     plt.subplot(num_rows, num_cols, 2 + i + x * num_cols)\n",
    "            #     if i == 0:\n",
    "            #         plt.ylabel('pred: ' + str(ind[0]) + '...', fontsize=15)\n",
    "            #     if x == 0:\n",
    "            #         plt.title(tit, fontsize=16)\n",
    "            #     p = viz.visualize_preds(scores[i], num=label_num, cbar=False) #axis_off=False,  vabs=vabs)\n",
    "            #     plt.xticks([])\n",
    "            #     plt.yticks([])\n",
    "            #     divider = make_axes_locatable(plt.gca())\n",
    "            #     cax = divider.append_axes(\"right\", size=\"2%\", pad=0.05)\n",
    "            #     plt.colorbar(p, cax=cax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    '''A simple conv net\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "    def logits(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def predicted_class(self, x):\n",
    "        pred = self.forward(x)\n",
    "        _, pred = pred[0].max(0)\n",
    "        return pred.item() #data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "im_torch, im_orig, label = dset.get_im_and_label(250, device=device) # this will download the mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = torch.tensor(mnist.XCnn[0].reshape(-1, 1, 28, 28)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 10)\n",
      "(10,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xiangyu/.conda/envs/PyTc/lib/python3.12/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/xiangyu/.conda/envs/PyTc/lib/python3.12/site-packages/acd/scores/score_funcs.py:70: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = F.softmax(model(input_vecs))[:, class_to_explain]\n"
     ]
    }
   ],
   "source": [
    "ACDexp = AcdExp(image, sweep_dim=1)\n",
    "score = ACDexp.get_explanation(model, mnist.y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 10)\n"
     ]
    }
   ],
   "source": [
    "score = score.reshape(28,28,10)\n",
    "print(score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 28, 28)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = np.moveaxis(score, -1, 0)\n",
    "score.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "colors = []\n",
    "for j in np.linspace(1, 0, 100):\n",
    "    colors.append((30./255, 136./255, 229./255,j))\n",
    "for j in np.linspace(0, 1, 100):\n",
    "    colors.append((255./255, 13./255, 87./255,j))\n",
    "red_transparent_blue = LinearSegmentedColormap.from_list(\"red_transparent_blue\", colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAADC8AAAEaCAYAAACL/ZzBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAujElEQVR4nO3daZhmZ1kn8OfUW1t3V2/pbrrT6XQIARICiWwSQmCMUVZBEFkEMugwoMiOIHEQveYSXEAFkSGiGBgFWQZkkN0RGI0sSSBGIIlJBEI66SSdTqe3qq71rTMfvMbxQ+a675jzVNVb/ft9/l/3/dR5T73nOef0Xd20bdsWAAAAAAAAAAAAAACASoaWewEAAAAAAAAAAAAAAMDqZngBAAAAAAAAAAAAAACoyvACAAAAAAAAAAAAAABQleEFAAAAAAAAAAAAAACgKsMLAAAAAAAAAAAAAABAVYYXAAAAAAAAAAAAAACAqgwvAAAAAAAAAAAAAAAAVRleAAAAAAAAAAAAAAAAqhpe7gUAAABADe/86tEw8+yz1qZq7dzQu7fL+Vcz822YufLWuTBz3iljXSznHpmcXQwzw0NNmDn97bel+r3x/A1h5hfOmQgzl988m+r3oo/fFWauee2JqVrfuT3+DB+wZSTMjI/ExzPrZZ+Mf75SSrn4GSd00q9t43O9lFJ+5sMHwsxHn7/13i4HgEH2tk/EmQvPz9Xa2c11rpRSyux8nLnihjDSPvbMDhbzL5omuXeYmokzw4k98Lb/mOv3lheEkfaVTw0zzWXX5/o9521xvz2XpEo13/5BHDrz5Dgzknsll9lDNT/3zlyt//7qVC7slz2vnvrmOPOZX8vVyuwls+saJMk9NHCcWY3fdwAAwLJLPYdyP8K/0dU5439eAAAAAAAAAAAAAAAAqjK8AAAAAAAAAAAAAAAAVGV4AQAAAAAAAAAAAAAAqMrwAgAAAAAAAAAAAAAAUJXhBQAAAAAAAAAAAAAAoCrDCwAAAAAAAAAAAAAAQFWGFwAAAAAAAAAAAAAAgKoMLwAAAAAAAAAAAAAAAFU1bdu2y70IAI5vB471w8yWtb0lWAkAsJpk9hgnrMnN9DdNc2+X86++vmc2zJy7e6yzfl266eBCmNm9Kd63Xbl3PtXvkbtGw8z93nZrmLn+dSem+o304s95vp97jJKp1V+Ma/WGcudeptbUfG7t60fjnm//ytEw87rHbUj1y/jWbXOp3Nk7RsJMl7/PACyRA/F1p2xZnyrV5SuR5mvXxf0ec0Zn/brU3LQ/DmX2IbcfSvVrf/j+YabZ+Ly40OEP5/olMk1i/1RKKW3iODTH4j1+WTee6lcy52imX7bnb30sjLT/5Vm5fgnNP96YCz7sfp31HChe2y6PzD2Cz4bl5D6WDnT5HGq1m12Ij9XYsGMFAAD/Xv7nBQAAAAAAAAAAAAAAoCrDCwAAAAAAAAAAAAAAQFWGFwAAAAAAAAAAAAAAgKoMLwAAAAAAAAAAAAAAAFUZXgAAAAAAAAAAAAAAAKoyvAAAAAAAAAAAAAAAAFRleAEAAAAAAAAAAAAAAKjK8AIAAAAAAAAAAAAAAFCV4QUAAAAAAAAAAAAAAKCqpm3bdrkXAQAAAF07OL0YZjavyc30f+m7M2Hm7BNHUrUef8n+MHPVK7eHmYX4xyullHJsPr7t3zi+tH/b4MhsbvHrR5sw0zRx5thcrt/a0e6OQ6bnSC9eeyZTSimnvPXWMPODN5yYqnXFLXNh5pyTx1K1ltr+qX6Y2bautwQrAaBTByfjzOaJXK2/virOPPL+uVrnvD7OfPeP40w/u7GbDSPtxHiqVGYPlXl91EzG++RSSinr1+Rykel4n1JKKWUosYcay+3fy4GjcWZtYm80nuy38Xlx5shHcrUuvz7OnHN6rtZSu/NInNm6of46lprXtsDdSVy3AYDB1l+M7wV6mXtdgBUs9bwxef8zk3gH3uvwFXj2nS05/ucFAAAAAAAAAAAAAACgKsMLAAAAAAAAAAAAAABAVYYXAAAAAAAAAAAAAACAqgwvAAAAAAAAAAAAAAAAVRleAAAAAAAAAAAAAAAAqjK8AAAAAAAAAAAAAAAAVGV4AQAAAAAAAAAAAAAAqMrwAgAAAAAAAAAAAAAAUFXTtm273IsAAACArh2bWwwzR2Zzt8Q71vfCzFdvmk3V2rwm/jsCD9o2HGYOzeTW/kufPRhm3v+sLalamUcITdOkamXM9+N+I7243y2HF1L9Msd096b4XCillLHEusaGuztWGTPzuXNmfCReV1efTdf6i/G6ekNLvy4A7qXpuThzeCpVqt2+Kcw0X7k2VaucsD7OPHh3nDk4mev3sveEkfZDr0uVyuzZunx91CSu0W3iGt3sPZBrePhYnLnvfXK1pmbCSLttY5jJ7pNTe+653P62jI3Emfm4Vjuc2wN3eV6ljlaH9x4rhte2wN1Zjd93AAAA98LhmfjfAmwc9/f9VyqfDAAAAAAAAAAAAAAAUJXhBQAAAAAAAAAAAAAAoCrDCwAAAAAAAAAAAAAAQFWGFwAAAAAAAAAAAAAAgKoMLwAAAAAAAAAAAAAAAFUZXgAAAAAAAAAAAAAAAKoyvAAAAAAAAAAAAAAAAFRleAEAAAAAAAAAAAAAAKjK8AIAAAAAAAAAAAAAAFBV07Ztu9yLAAAAgK7NLsS3u5Nzi6laU3Nxrd2bhlO1Lr95Nsycc/JYmDmWXPva0fjvFnT5aKBpmjCT+WxKKeXWI/0wMzEW99u2rpfqd8vhhTDzl1dPp2p945a5MPP+Z50QZkZ68c+X1V/MHffeUNwzc84k26X6/cyH70zV+vqe+LjfdNHOVC0AVpCpmTiz/0iu1mK8h2pP3Z4q1Vx2fRw694w4Mx1fv0oppR0fCTOZvVhW5nrfLMT7tVJKKTcnruVr4z1w2bE51++WRL8PXZqr9fXE5/yR14eRdjR3v5CR/pQ7PB8yUufM096SK3bpNXHmyEdytQZJ4hgem89ttDP3gpl+wAqwxN/nAEBO6h7IdZxVLPPeKfMOqBS/T9ThvFq5/M8LAAAAAAAAAAAAAABAVYYXAAAAAAAAAAAAAACAqgwvAAAAAAAAAAAAAAAAVRleAAAAAAAAAAAAAAAAqjK8AAAAAAAAAAAAAAAAVGV4AQAAAAAAAAAAAAAAqMrwAgAAAAAAAAAAAAAAUJXhBQAAAAAAAAAAAAAAoKqmbdt2uRcBAAAAy+HaO+ZTuQdsGQ4zx+Zzt9fX7Y97rh+L/9bAfTf1Uv1Gek2YGYojpZRSFhbjTKbWp6+bTvX7idPXhJmHv+v2MPPtV+9I9bvtaD/M7NoYnwullHLrkbjW9on4c+5lP5wO3XJ4IcxccctcmHn6g+LPr5Tcz3hsLnHylVLWjvo7HQDHrWv25HJn7IozV9+UqzUTXw/L+GiceeBJuX7DievccG6PWGYS++DMPuSvLs/1+6lHx5kdPxtn7vxgrt/eA2GkPWlLqlRz28E4dOLmuF+qW7eaxHEoX7suzjz7vFS/zM/YZM69Uko7PhLXapZ+r1yd17bA3VmN33csuf5ifI1ZjudQQDc8PwWOBwenc991m9fE33WZfzadfe7QZa2VuGebSbwDHx+xj+SesSMBAAAAAAAAAAAAAACqMrwAAAAAAAAAAAAAAABUZXgBAAAAAAAAAAAAAACoyvACAAAAAAAAAAAAAABQleEFAAAAAAAAAAAAAACgKsMLAAAAAAAAAAAAAABAVYYXAAAAAAAAAAAAAACAqgwvAAAAAAAAAAAAAAAAVTVt27bLvQgAAADoWuZ2t2maVK2Z+bjW+EiuVsaeQwthZvem4VSt+X689pFebu3v/+ZkmPmx+4+Hmfus66X6ZY7pJ689FmYOTC2m+j377LVhZnYh9xhlW+Jn/MQ18dqf+eB4TVnv/OrRVO6pZ6wJM6dtyZ1/GT/9wTvDzF9euDVVq8vfewBWkDsOhZF228ZUqWYu3meVsZFUrdR15+b4Old2b0v1K/14T9MO5a5zzZ/8dRx68iPizPZNqX7taLx3aD721bjQgdx+pjz7vFwuod2yPsw07/lCXOelT0r1S+1V3vaJVK3yzHPjzP1PzNXKeNyvhJH20t9OlWruiu89SuKzGThe28JAmE88Zhjp8s9Iuo8FgBVpkJ9HH5mNNzQbxvxd7HtikM+Hf68Dx/phZsva3DtBjg9dvrfOOB5/L2s7NhdfP9aOxtcPVxgAAAAAAAAAAAAAAKAqwwsAAAAAAAAAAAAAAEBVhhcAAAAAAAAAAAAAAICqDC8AAAAAAAAAAAAAAABVGV4AAAAAAAAAAAAAAACqMrwAAAAAAAAAAAAAAABUZXgBAAAAAAAAAAAAAACoyvACAAAAAAAAAAAAAABQleEFAAAAAAAAAAAAAACgqqZt23a5FwEAAAAr2SlvvTXMXPHy7ala2yd693Y5pZRSjs0tpnK3HO6Hmd2bhlO1xhKxqbn4McPEWO5vKVx82dEw88KHreusX+Zz/tNnnpCq9cCt8cE6ZXPuuHcl+wioaZowc3gmPv/WjsR1SillpBfn+ou5tfeGcj0BOI5t+Jk48/0/SZVqt6wPM5nrapmeS/Ure/bHmfveJ1drbCTOTM2EkXbtWKpd885Px6GXPKG7fhufF4c+dlGqVjn9pDiTOO5d7sWyMj2bI9NxoYnxXL/EXiz702WOVpfHasXw2ha4O6vx+w4AAIDjlv95AQAAAAAAAAAAAAAAqMrwAgAAAAAAAAAAAAAAUJXhBQAAAAAAAAAAAAAAoCrDCwAAAAAAAAAAAAAAQFWGFwAAAAAAAAAAAAAAgKoMLwAAAAAAAAAAAAAAAFUZXgAAAAAAAAAAAAAAAKoyvAAAAAAAAAAAAAAAAFTVtG3bLvciAAAAYDlMzi6mchNj3c3+Z27Dr943H2ZO3Tyc6tfl2uf78dqHmrhOLxMqpcwuxP3GhnO1MjI/30ivu379xbhf5hiUkjsOycNefvfSo2HmVY9ZH2bGR7o7Vll7Di2Emd2bcr87AKwcmf1Tc2gqV2vTurhW0+E17MrvxZkzTkqVateOhZn02ufja2YZ7oWR7AumZv/huNa2jclqiX4L/Tg0ktsTpM6/TKG5xDEvpZSxkVwu4y3/I868+mlhpJ0YT7XLnH/Z15LN3gNxaNfWVK2B4rUtcHe63JtAB44knqlu6PCZJKxWmefDpeSfpwOQl3re0+E+vMt/pt3ps0tWrMw+YZD3CO4WAAAAAAAAAAAAAACAqgwvAAAAAAAAAAAAAAAAVRleAAAAAAAAAAAAAAAAqjK8AAAAAAAAAAAAAAAAVGV4AQAAAAAAAAAAAAAAqMrwAgAAAAAAAAAAAAAAUJXhBQAAAAAAAAAAAAAAoCrDCwAAAAAAAAAAAAAAQFWGFwAAAAAAAAAAAAAAgKqatm3b5V5Ebf3F+EfsDTVLsBIAAAAGUebWuWly95V7Di2EmV/7m8Nh5s+evSXV76pb58LMw3aOpmp15YnvuyOV+9zPbQszmfv5W4/0U/12builchm3H417/saX4s/5zU/YmOq3ZW289psOxudeKaWcsnk4zGSetWQftSwsxpmRnuc2MKi6vIZy/Fqx59Ge/XHmRX8YZ7745ly/K78XZ844KVWqXTuW6xloHnNRLvi1t4aRzMuqZu+BVLv2pNxeOaPZdygOvfAP4syfviLVrz15a5hpbtyXq3Xq9rhWplD296sfb+zaDt/Hrcrrx+p/bduZY/O5Y7V2ZBWeJ8sp83vnPO7eavy+gxVsdiH+Hhsb9nsJDLb5fvxd1+Vz+ew/UV2V93msSMfm4mcYa0cH92/DL/Xv3Er9Hc9812UypQz2+bASOZoAAAAAAAAAAAAAAEBVhhcAAAAAAAAAAAAAAICqDC8AAAAAAAAAAAAAAABVGV4AAAAAAAAAAAAAAACqMrwAAAAAAAAAAAAAAABUZXgBAAAAAAAAAAAAAACoyvACAAAAAAAAAAAAAABQleEFAAAAAAAAAAAAAACgqqZt23a5FwEA0LUjs4up3IYxs5zAYPvMP02nck990JrKKxlM8/3cLfFIrwkz37xlLlXrkbtGU7lIfzG39oXEJTH7ZODdlx0NM685b32Y6Q3Fx7OUUt7xlSNh5rWP3RBmbjm8kOq3bV0vzIwN59aekXkk0zS5fjfsnw8zW9bl9j3z/TjTS5TaNJ7r10+co4eTe7vvHog/6/NOGUvVAlhq2Uf12WvDapK6Zib3Rm1iH9J847upWuVRD8jlItnN2ELiIp08DuV3/2ecedNzwkj6vH3rJ+JaFz0zrrP3QKpf2b4p7jcc7/2yOv29vH5vnNka74FLKaX0E+fMUGLPtnki12/foTiTOfdKKeWZj44zj3twrtYg8doWVo353G1sGcncOh+H+z9gMLiPBY4HvutYSl2+O4T/6+B0fIO6ec3S/vs5/1oPAAAAAAAAAAAAAACoyvACAAAAAAAAAAAAAABQleEFAAAAAAAAAAAAAACgKsMLAAAAAAAAAAAAAABAVYYXAAAAAAAAAAAAAACAqgwvAAAAAAAAAAAAAAAAVRleAAAAAAAAAAAAAAAAqjK8AAAAAAAAAAAAAAAAVNW0bdsu9yIAAO6JzPalaZolWAkAK9mNdy2EmV5ypH/3puEw8+f/MJWq9Zyz1oaZ8ZHurmM37J8PMw/cNtJZv8tvng0zj9o1mqrV1fU8++hjYTHOfC9xXpVSyhmJY7p/qh9m1iXPhTWJ3ErdH910MD6mp2yOfwdLKWVyNv4QJ8b8LQ+AgfP92+PMcC9Vqj15a5hpLv58qlZ58ePjfqO5a1hGc8Otcb8H7uyu32XXx6Fzz+isX6fPe/qJjd0Ne3O1HnRynLnzSJxZk9sDl3XjuVxCV8c0u59ubtwX1zp1e67Wsfi+ostjtWJ4bctqkP2udr7nrdD7eQBgdUvfC9qrsERmF3Ln5NiwcxJWOm9rAQAAAAAAAAAAAACAqgwvAAAAAAAAAAAAAAAAVRleAAAAAAAAAAAAAAAAqjK8AAAAAAAAAAAAAAAAVGV4AQAAAAAAAAAAAAAAqMrwAgAAAAAAAAAAAAAAUJXhBQAAAAAAAAAAAAAAoCrDCwAAAAAAAAAAAAAAQFWGFwAAAAAAAAAAAAAAgKqatm3b5V4ELJXs6d40TeWVANTjuw4AlsfsQu4aPNqLM0dm41obx7v7ewQPevttqdzVr9kRZj5xzXSYue/mxEEopTxo20iYGenFe5qh5LbnDZ8/FGZ+7ymbUrV+cLAfZk7bMhxmsp/NHz5tc5h5/APGU7WW2uU3z4aZB98nPhdKKWXdaPxh2wcDDJ4uX2OkrgPzC7liI/G1vBw4GkbaEyZy/RKaXS/KBfe+P868/4tx5syTc/0eckqcGR+NMx/821y/r18XZ/7oF3O1vnd7GGlPi/fJzcbn5fp99Jfjfk96eK7WEmu+ljjup5+UK/axr8aZX3xyrtYgGeTXtpnv10H++aCS2ycXw8yO9bnnKDBoPnVt/ByxlFJ+8sw1lVdyzx2bi393Syll7ai/KQsw6Gbmc/cxX9sTv+u44LSV+Z6GwZJ5Jz02vLTvwrLvyZd6XaxcdskAAAAAAAAAAAAAAEBVhhcAAAAAAAAAAAAAAICqDC8AAAAAAAAAAAAAAABVGV4AAAAAAAAAAAAAAACqMrwAAAAAAAAAAAAAAABUZXgBAAAAAAAAAAAAAACoyvACAAAAAAAAAAAAAABQleEFAAAAAAAAAAAAAACgqqZt23a5FwEAAAArWebWuWmaVK1f+cKhMPM7T9oUZm7YP5/q974rp8LMG390Q6rWupH4Z+wNxZnZhdyjiJlEbt/Rfpi5KHHMSynlExduTeUy/tc/z4SZJz5wTZiZnF1M9ZsY6+7vU2TO97d/5WiYed3jcudVl47Nxcdr7ai/5QEwaLp8jZHZs2X7Na+9JA79wYvjzPV7U/3Kuz8bZ377hbla68Zzucj8Qi53bC7O3H4wzrz6vbl+X/ivuVzG574ZRtonPyLMNJPx/rCUUtqJjj6bkrxH+a2PxZk3PjvVr8v7pjKbuN8ZG8nVGiRe2wJ3J/vdCQAAAAPA21oAAAAAAAAAAAAAAKAqwwsAAAAAAAAAAAAAAEBVhhcAAAAAAAAAAAAAAICqDC8AAAAAAAAAAAAAAABVGV4AAAAAAAAAAAAAAACqMrwAAAAAAAAAAAAAAABUZXgBAAAAAAAAAAAAAACoyvACAAAAAAAAAAAAAABQVdO2bbvciwAAAIDlcGxuMZW77WicO23LcKrW7EJ8G/7+KyfDzEvPWZ/q11+M+/WGmlStrnz5ezOp3AWnjYeZzM+XNTkX12qSh+o3vnQ4zPzeUzaHmX2T/VS/7RO9VC5jvh8fh5Fed+dM5nzInAullLLn0EKY2b0p97sKwMqReY3RzMznit12V9zv1O2pUk1mH/Luz8b9XvnUXL9UqLtrdOq4f/FbuVo//kNxrUyh7M93dDrOZPfAF/1ZGGnf9fNhpjlwNNWu3ZLb52cs9TlTPnlZGGmffk6qVLP3QBzatTVVa6B4bQvcnS6/qwEAADiuZP5NxNrRpf2/EPzPCwAAAAAAAAAAAAAAQFWGFwAAAAAAAAAAAAAAgKoMLwAAAAAAAAAAAAAAAFUZXgAAAAAAAAAAAAAAAKoyvAAAAAAAAAAAAAAAAFRleAEAAAAAAAAAAAAAAKjK8AIAAAAAAAAAAAAAAFCV4QUAAAAAAAAAAAAAAKAqwwsAAAAAAAAAAAAAAEBVTdu27XIvgqWR+aibplmClQAAAAyW1332YJh50wUbU7X+Ye9cmPmx+4+nanXl89dPp3JPemC8ri9/bzbMfP+uhVS/lzxqIpXrSua++Zo7cmt/yPaRTvpNzeUe20zNx7mta3N/w6I3FD8b+Ox18TnzlNNz53HmWcThmcVUrT+6bDLM/Mr5G1K1AFYqz3nvXvZVR/PyP45Dv3lhrukV/xxG2ic8NFerI82nv5EL/uSj4sxfXxVnvvODXL9XPjWMtKPDuVoJqd+B79yUqtU+ZHfcL1NoZj7Vrxw9Fme25e49Mr8VqXMmc75kHc3de5SnvjnO/N1v3bu1rERe2w681O9d9VWw6hyHezuWx+xC7jo0Nuyc5J659o54L3zmfeLnqQAA94Rn6d3LvLPdOB6/k/Y/LwAAAAAAAAAAAAAAAFUZXgAAAAAAAAAAAAAAAKoyvAAAAAAAAAAAAAAAAFRleAEAAAAAAAAAAAAAAKjK8AIAAAAAAAAAAAAAAFCV4QUAAAAAAAAAAAAAAKAqwwsAAAAAAAAAAAAAAEBVhhcAAAAAAAAAAAAAAICqmrZt2+VeBADAPZHZvjRNswQrAYA63nvFZJh50SPXpWr1huJrYn8x92ggU6tLmXUdmlkMM390WXw8SynlTRdsTOUyMmufmo8zG8Zyf3cisz/6q3+aTtV6xplrU7nI7Uf7qdxcP177SRt6qVrziZbjI/aJAINmpb7GSD17ePfn4szLnpxtGGeSx2qpj2jq6ntXYs/2svek+rUfeX0ql5Fa++RMGGknxnP9Mp/zx7+WqtX+9LmpXKS57WAuODMXZ07dnqs1txBnxkZytQbJCv2+A5aZ9x0AAMAKduNdiec4pZRTTxiuvBIGhf95AQAAAAAAAAAAAAAAqMrwAgAAAAAAAAAAAAAAUJXhBQAAAAAAAAAAAAAAoCrDCwAAAAAAAAAAAAAAQFWGFwAAAAAAAAAAAAAAgKoMLwAAAAAAAAAAAAAAAFUZXgAAAAAAAAAAAAAAAKoyvAAAAAAAAAAAAAAAAFRleAEAAAAAAAAAAAAAAKiqadu2Xe5FAAAAQNcOzyyGmbFek6o1vRDfOm9ek/v7AAen43Vla2Xsm+yHmROS/S6/eS7MXLk3zrzqMROpfk0Tfz6v/NTBMPOHT9uU6rcQfzRlMfkUZWw4Xns/UWwod4qWuxLn1Za1vVyxjnzzlvhcKKWUR+4a7azn5TfPhplzTh7rrB8AS+TgZJwZHc7VSlzw2w1rUqWaQ1NxrU3rUrVS/fYfjkPbNuaKXXpNnLn8hjjzhmem2mVeRTUveXdc6E9fkepX+pmNXSJTSmmH4z1UZt+aljjfuzyvMprLrk/l2kef3l3Pr1wbhx734M76rRhe2wJ3p8vrDAAALKHsP0/u9NkKHCcy75pLKaWXfeHcUc9MP//zAgAAAAAAAAAAAAAAUJXhBQAAAAAAAAAAAAAAoCrDCwAAAAAAAAAAAAAAQFWGFwAAAAAAAAAAAAAAgKoMLwAAAAAAAAAAAAAAAFUZXgAAAAAAAAAAAAAAAKoyvAAAAAAAAAAAAAAAAFRleAEAAAAAAAAAAAAAAKiqadu2Xe5FAAAAQNde+5mDYebtP7EpVatpmnu5mv9nZj6+Db9633yYecDW4VS/jePx3y3oL+YeDfSGujkO2UcRmWVl1jS7kOv32D/eF2aueNn2VK3MOfOOrxwJM685b31n/S6/eTZV64Q18Tmz90g/zJx/v/FUv8z58Hc35tae7QnAgLngTXHmy29Jlcpcd9J7v6mZOPPtH8SZM3en2rUb1oSZ7G6tqxdD6WN1dDqMtBPxdbxZiPcgpZRSznpVnEmeM2XnCXHmd/4yzlz0zFS7zGfTXHZ9qlZZOxZn7poMI+35D0m1S50PX/pWqlZ7wdnd9BswtxxeCDO7NvS6a5g9hl4nD7QD04up3JbEvSDLZBV+361Ene4TARh4mXc54yOuCzCo7P1YTpl312PDq/v88wQCAAAAAAAAAAAAAACoyvACAAAAAAAAAAAAAABQleEFAAAAAAAAAAAAAACgKsMLAAAAAAAAAAAAAABAVYYXAAAAAAAAAAAAAACAqgwvAAAAAAAAAAAAAAAAVRleAAAAAAAAAAAAAAAAqjK8AAAAAAAAAAAAAAAAVNW0bdsu9yIAIDK7kLtcjQ03lVcCAKwmew4tpHK7Nw2HmZsO5mptWRv/HYGJscH9WwMz8/G+7ekf2J+q9Zmf3RZmHv6u28PMd15zYqrffD9e+9X75lO1HrZzNJWLZB/bHJqJc5vGc3vlxUTLz18/E2Z++OTcMdia+J1YWEyVKqO9ONM07hmA7mSue6WUMj7iu+feyFwPm5vvzNU6eWtc66bcXqVs2xBn1o2Hkez1vstrWOqYziX2t2e9Ktfw+ovjzCkvjjN7Lsn1m0+s/aobU6XaH75/rmcg/fkdnIwzmyfu3WL+rU9eFmf2HcrV+vknxpkDR3O1tiZ+v1Yjr22Bu+M+liXi/ub4cGQ296BtwwA/owboZ150lFJ6Q65psJRSzyTd/6Rde0fuvfWZ9xkJM0u9R7TTBAAAAAAAAAAAAAAAqjK8AAAAAAAAAAAAAAAAVGV4AQAAAAAAAAAAAAAAqMrwAgAAAAAAAAAAAAAAUJXhBQAAAAAAAAAAAAAAoCrDCwAAAAAAAAAAAAAAQFWGFwAAAAAAAAAAAAAAgKoMLwAAAAAAAAAAAAAAAFUZXgAAAAAAAAAAAAAAAKpq2rZtl3sRAAAA0LX+Yny7+wdfPZqq9Ypz14eZseEmVWvfZD/MfOgfp8LMD504mur3o/cbS6xpMVVrx/peKhfJfDallNIbio9pplamTtdmF+J1ffLa6TDz3LPXdrGcUkopv3vpkVTul//DhjBzwXvvCDNfevG2VL+MK26ZS+VO2TQcZro6jwFYQpnXGL/98VytX/6pODMSX09KKaXccSjOvO9LceYRp+X6Pf6hcSazplJKuc+mMJJ5fdQ0uX1WqtZCvE/OfjZdvvpqMnvXj1waRtrn/0gHq/kXza9/KBd88wvizH1fEmd+8N5cv4yvX5fLnbo9zuzYfO/WshJ5bQvcneT1FgBYWl3eNwMw+Ob7uec6I7342nBwOn5/v3nN4P7/BYO7cgAAAAAAAAAAAAAAYCAYXgAAAAAAAAAAAAAAAKoyvAAAAAAAAAAAAAAAAFRleAEAAAAAAAAAAAAAAKjK8AIAAAAAAAAAAAAAAFCV4QUAAAAAAAAAAAAAAKAqwwsAAAAAAAAAAAAAAEBVhhcAAAAAAAAAAAAAAICqmrZt2+VeBAAAACyH7C1x0zSd9dw32Q8z2yd6YeZv/nkm1e+sHSNhZsf6uF9W5phmj+d3bp8LM7s2DoeZDWO5fkPdfcypn/Hiy46GmQtOG0/1O2Nb/Dnvn4rPvVJKOWFN/Lcu5hOl3nbpkVS/X/+xjakcAHShy1cizW0H49DOE+LMp67INXz4/eLMrq2pUl3t2dL76W//IA7dd3uc2bAm1a90uH9PHat3fjou9MSH5fqdsSvudyDeR5ZSStmyPs4sJDZ2v/HRXL83vyCXS+jyvmKgrNTXtplj3eXal7ofrHSr8fsOgGWx5PeCrmHAAPNdB/X4nxcAAAAAAAAAAAAAAICqDC8AAAAAAAAAAAAAAABVGV4AAAAAAAAAAAAAAACqMrwAAAAAAAAAAAAAAABUZXgBAAAAAAAAAAAAAACoyvACAAAAAAAAAAAAAABQleEFAAAAAAAAAAAAAACgKsMLAAAAAAAAAAAAAABAVYYXAAAAAAAAAAAAAACAqoaXewEAAABQw3y/DTMv/sRdqVpPOX1NmHnu2WtTtTaNd/N3BM4+cSSV2z7R66Tfctg/tRhmztqxtH+XoW3j8yrrBQ9dF2YmRptUrf/29aNh5kWPiPuVUkpvKO5529GFMDO/mDtWV++bDzMP2pZ7hJVZOwADaD6+7pTn/36qVPOMc8JM+/wfSdUqmydyuchDT03F2pO2hJmlvhI2TbLjHYfDSHv2fTvrl9mzpdee8Z9+PM4M5/atzTs+FYd+8cmpWiXzM265MM78wpNy/a7ZE2fOPDlXi5Wlw/ugFdkPAOA40dV9UKf3U6xY2fchS30+3H60H2Z2rB/cd2OsHAvxa8pSSikjTreB1k++z1zqd5A/+ef7w8ynXrhtCVZSh/95AQAAAAAAAAAAAAAAqMrwAgAAAAAAAAAAAAAAUJXhBQAAAAAAAAAAAAAAoCrDCwAAAAAAAAAAAAAAQFWGFwAAAAAAAAAAAAAAgKoMLwAAAAAAAAAAAAAAAFUZXgAAAAAAAAAAAAAAAKoyvAAAAAAAAAAAAAAAAFTVtG3bLvciACCSvVw1TVN5JQDAoJicXQwza0Zye4ep+Xgvcsk3JlO1XvvYDWFmJtFvPLn2lejCjx5I5T743C1hJrNPXI494nw/cc58Mz5nXnrO+lS/I4nzff1o7jgcnYvXnvn5tqztpfplfPTbx1K5Zz1kTZjpDQ3u7w7AcWt6Ls6sGc3VOpy4pvzqB1Kl2nf9fJhpZubjOuMjqX6ZPU2Xr3xSe6in/2au2F/96r1bzD3U6XHIhN79uTDSvvwpuX6TM3FofbznKaWUcnQ6ziz0w0i7aV2qXeqc+Yu/TdUqz/+RTMNcrUHitS1wd1bj9x0Anbr1SLyvL6WUnRu6e2YLAPDv5X9eAAAAAAAAAAAAAAAAqjK8AAAAAAAAAAAAAAAAVGV4AQAAAAAAAAAAAAAAqMrwAgAAAAAAAAAAAAAAUJXhBQAAAAAAAAAAAAAAoCrDCwAAAAAAAAAAAAAAQFWGFwAAAAAAAAAAAAAAgKoMLwAAAAAAAAAAAAAAAFUNL/cCACCjaZrlXgLAitK2bZjx3cnxbmKsu3n9g8f6YWZDh/3uTPTbtTF3S99fjL8vekPdfV9kvp8ufsbmVK3J2cUw8w+3zoWZs3aMpvptXtPdZ/jp66bDzDu+MhlmXnrO+lS/6fn4uB+Yio9nKaVsn4iPw2zcLu3z18fH6jlnrUnVcu0DWKXW5K7lKQfj6285NttdvwNHwkiza2uuVmKfteQ++Eu53HdvizM33xln/v7aXL9fe04ul/Hhv48zb/xAGGle8RO5frPzcWb/4VytE0/opF96j/WZb8SZF5yfqwUArFiZZ3ZdPpsFYjs39JZ7Cfx/fPCqqVTuwoet66xn5j3NoZk40+U7E+jCVYl3gqWU8rCdHT5LXOXm+/F3wUgvfi6UqZOtleXfzAw2VxgAAAAAAAAAAAAAAKAqwwsAAAAAAAAAAAAAAEBVhhcAAAAAAAAAAAAAAICqDC8AAAAAAAAAAAAAAABVGV4AAAAAAAAAAAAAAACqMrwAAAAAAAAAAAAAAABUZXgBAAAAAAAAAAAAAACoyvACAAAAAAAAAAAAAABQleEFAAAAAAAAAAAAAACgqqZt23a5F3F3sstqmmYg+wEAAFDXzHx8nzc1v5iq9f27FsLMw3eOpmpNJ9Y1lLj1fMv/PpLqNzUX93vFuROpWg/YOhJm5vtxv37usJde4k8uLCZu58eGV+a9/ClvvTXM3HTRzs769TMHq5Ty8aunw8xzz157b5fzr24/2g8zNx6MfwdLKeXc3WP3djkArESz83Hm2Gyu1rU3x5kHnJirNZTYrKwbjzNnvTLX74xdceYd/zlVqr1//DM2mb3DzFyqXxnuxZnEJrjN1Cm5dzldvh5rNj4v7nf4w931ywY//PdhpH3e4+J+2Xdj3789zuw7lCrVPvr0MLMq39kt8WvbycT9YimlTIyuwmMNg2Q1ft91JPOso5d50NZxLVgNMs95R3p+JwBYeTLPfDLPFL703ZlUvwdvj9+f7life6bVlexzr8zjP3vgwdfV70SX/M8LAAAAAAAAAAAAAABAVYYXAAAAAAAAAAAAAACAqgwvAAAAAAAAAAAAAAAAVRleAAAAAAAAAAAAAAAAqjK8AAAAAAAAAAAAAAAAVGV4AQAAAAAAAAAAAAAAqMrwAgAAAAAAAAAAAAAAUJXhBQAAAAAAAAAAAAAAoKqmbdt2uRcBdCfzK900zRKsBKCeg9OLYWbzGjOaAMe7ydn4ejExlrte3O9tt4aZ779hZ6rWfD/es4/04j37l783k+p33iljYWYoeYtwbD5e+xMuuSPMXP7yHbmGCTfsnw8zG8Zzn/N04ufbvamXqpXRSxz4/VP9VK1t6+J13XJ4IVVr+0RcK3OOZn4HSynlfVdOhZlXPWZ9qhbAStVfjK8xmevCcevwsTDSbliTKtVsvTAOHfiLVK3Sj691beJzbb74rVy/8x8SZ4aSz0MmE3vJ894QRtrvvCvXL6G5Id5zl5Hu9mLlfsk9aeKZe+ZFW3PgaK7dlnjf0+w9kKpVtm2MM2MjcWZ6Ltfv2W8NI+2n35SrlbAq33V4bQsDIfW932XDVfZ9Z28Mg6Gr59gAK9nhmfjZzsbkOyY4ns0u5J5njA3bOyyHlbiv880KAAAAAAAAAAAAAABUZXgBAAAAAAAAAAAAAACoyvACAAAAAAAAAAAAAABQleEFAAAAAAAAAAAAAACgKsMLAAAAAAAAAAAAAABAVYYXAAAAAAAAAAAAAACAqgwvAAAAAAAAAAAAAAAAVRleAAAAAAAAAAAAAAAAqjK8AAAAAAAAAAAAAAAAVNW0bdsu9yIAAACgazcdXAgzOzf0UrVGek2Y6S/mbq8zsUw/lsfLPnlXKnfxM04IM++9YjLMvORRE6l+GbMLuXN0bDg+/y75Rrz2Fz58Xapf5nw/OL2YqvWBq6bCzKsesz5VC45n2UfGTeN6xRLZsz/OnLQlV6vX4d90yvyuJH5PBvk1TfZ7YKl/xtS6XvD7qVrtB38p7nfx5+NCL39Krl/iWDUL/Vyt4fh+p3nPF+JCL31Srl8i0xyK92ullFIu+Zs48/qfytUaJAP8fQBUZN8NAAAcJ7yfOD74nxcAAAAAAAAAAAAAAICqDC8AAAAAAAAAAAAAAABVGV4AAAAAAAAAAAAAAACqMrwAAAAAAAAAAAAAAABUZXgBAAAAAAAAAAAAAACoyvACAAAAAAAAAAAAAABQleEFAAAAAAAAAAAAAACgKsMLAAAAAAAAAAAAAABAVU3btu1yLwIAAAAAAAAAAAAAAFi9/M8LAAAAAAAAAAAAAABAVYYXAAAAAAAAAAAAAACAqgwvAAAAAAAAAAAAAAAAVRleAAAAAAAAAAAAAAAAqjK8AAAAAAAAAAAAAAAAVGV4AQAAAAAAAAAAAAAAqMrwAgAAAAAAAAAAAAAAUJXhBQAAAAAAAAAAAAAAoCrDCwAAAAAAAAAAAAAAQFX/BwJTLeJ3sNIEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 4000x1000 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(40, 10))\n",
    "\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    # plt.imshow(-bhem_exp.image, cmap='gray', alpha=0.3)\n",
    "    plt.imshow(score[i], cmap=red_transparent_blue, vmin=-np.nanpercentile(score, 99.9),vmax=np.nanpercentile(score, 99.9))\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_scores(im_torch, im_orig, label_num, model, preds, sweep_dim):\n",
    "    '''Computes different attribution scores\n",
    "    '''\n",
    "    scores = []\n",
    "\n",
    "    # cd\n",
    "    method = 'cd'\n",
    "    tiles = acd.tiling_2d.gen_tiles(im_orig, fill=0, method=method, sweep_dim=sweep_dim)\n",
    "    scores_cd = acd.get_scores_2d(model, method=method, ims=tiles, \n",
    "                                   im_torch=im_torch, model_type=model_type, device=device)\n",
    "    scores.append(scores_cd)\n",
    "    for method in ['occlusion', 'build_up']: # 'build_up'\n",
    "        tiles_break = acd.tiling_2d.gen_tiles(im_orig, fill=0, method=method, sweep_dim=sweep_dim)\n",
    "        preds_break = acd.get_scores_2d(model, method=method, ims=tiles_break, \n",
    "                                            im_torch=im_torch, pred_ims=dset.pred_ims)\n",
    "        if method == 'occlusion':\n",
    "            preds_break += preds\n",
    "        scores.append(np.copy(preds_break))\n",
    "    \n",
    "    # get integrated gradients scores\n",
    "    scores.append(acd.ig_scores_2d(model, im_torch, num_classes=10, \n",
    "                                           im_size=28, sweep_dim=sweep_dim, ind=[label_num], device=device))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pick an image + get scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_nums = [34, 20, 36, 32] # 34: screen, 20: snake, 36: trash can, 32: crane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (5x320 and 1600x100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m ind \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margpartition(preds, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m8\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m8\u001b[39m:] \u001b[38;5;66;03m# top-scoring indexes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m ind \u001b[38;5;241m=\u001b[39m ind[np\u001b[38;5;241m.\u001b[39margsort(preds[ind])][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# sort the indexes\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m scores \u001b[38;5;241m=\u001b[39m get_diff_scores(im_torch, im_orig, label_num, model, preds, sweep_dim)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# plot raw image\u001b[39;00m\n\u001b[1;32m     12\u001b[0m num_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(im_nums)\n",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m, in \u001b[0;36mget_diff_scores\u001b[0;34m(im_torch, im_orig, label_num, model, preds, sweep_dim)\u001b[0m\n\u001b[1;32m      7\u001b[0m method \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m tiles \u001b[38;5;241m=\u001b[39m acd\u001b[38;5;241m.\u001b[39mtiling_2d\u001b[38;5;241m.\u001b[39mgen_tiles(im_orig, fill\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, method\u001b[38;5;241m=\u001b[39mmethod, sweep_dim\u001b[38;5;241m=\u001b[39msweep_dim)\n\u001b[0;32m----> 9\u001b[0m scores_cd \u001b[38;5;241m=\u001b[39m acd\u001b[38;5;241m.\u001b[39mget_scores_2d(model, method\u001b[38;5;241m=\u001b[39mmethod, ims\u001b[38;5;241m=\u001b[39mtiles, \n\u001b[1;32m     10\u001b[0m                                im_torch\u001b[38;5;241m=\u001b[39mim_torch, model_type\u001b[38;5;241m=\u001b[39mmodel_type, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     11\u001b[0m scores\u001b[38;5;241m.\u001b[39mappend(scores_cd)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mocclusion\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuild_up\u001b[39m\u001b[38;5;124m'\u001b[39m]: \u001b[38;5;66;03m# 'build_up'\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/PyTc/lib/python3.12/site-packages/acd/scores/score_funcs.py:171\u001b[0m, in \u001b[0;36mget_scores_2d\u001b[0;34m(model, method, ims, im_torch, pred_ims, model_type, device)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ims\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):  \u001b[38;5;66;03m# can use tqdm here, need to use batches\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m         scores\u001b[38;5;241m.\u001b[39mappend(cd(im_torch, model, np\u001b[38;5;241m.\u001b[39mexpand_dims(ims[i], \u001b[38;5;241m0\u001b[39m), model_type, \n\u001b[1;32m    172\u001b[0m                          device\u001b[38;5;241m=\u001b[39mdevice)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m    173\u001b[0m     scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqueeze(np\u001b[38;5;241m.\u001b[39marray(scores))\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbuild_up\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/PyTc/lib/python3.12/site-packages/acd/scores/cd.py:59\u001b[0m, in \u001b[0;36mcd\u001b[0;34m(im_torch, model, mask, model_type, device, transform)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# deal with specific architectures which have problems\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmnist\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cd_propagate_mnist(relevant, irrelevant, model)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet18\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cd_propagate_resnet(relevant, irrelevant, model)\n",
      "File \u001b[0;32m~/.conda/envs/PyTc/lib/python3.12/site-packages/acd/scores/cd_architecture_specific.py:65\u001b[0m, in \u001b[0;36mcd_propagate_mnist\u001b[0;34m(relevant, irrelevant, model)\u001b[0m\n\u001b[1;32m     63\u001b[0m relevant \u001b[38;5;241m=\u001b[39m relevant\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m320\u001b[39m)\n\u001b[1;32m     64\u001b[0m irrelevant \u001b[38;5;241m=\u001b[39m irrelevant\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m320\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m relevant, irrelevant \u001b[38;5;241m=\u001b[39m propagate_conv_linear(relevant, irrelevant, mods[\u001b[38;5;241m3\u001b[39m])\n\u001b[1;32m     66\u001b[0m relevant, irrelevant \u001b[38;5;241m=\u001b[39m propagate_relu(relevant, irrelevant, F\u001b[38;5;241m.\u001b[39mrelu)\n\u001b[1;32m     68\u001b[0m relevant, irrelevant \u001b[38;5;241m=\u001b[39m propagate_conv_linear(relevant, irrelevant, mods[\u001b[38;5;241m4\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/PyTc/lib/python3.12/site-packages/acd/scores/cd_propagate.py:16\u001b[0m, in \u001b[0;36mpropagate_conv_linear\u001b[0;34m(relevant, irrelevant, module)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Propagate convolutional or linear layer\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mApply linear part to both pieces\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mSplit bias based on the ratio of the absolute sums\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     15\u001b[0m device \u001b[38;5;241m=\u001b[39m relevant\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 16\u001b[0m bias \u001b[38;5;241m=\u001b[39m module(torch\u001b[38;5;241m.\u001b[39mzeros(irrelevant\u001b[38;5;241m.\u001b[39msize())\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     17\u001b[0m rel \u001b[38;5;241m=\u001b[39m module(relevant) \u001b[38;5;241m-\u001b[39m bias\n\u001b[1;32m     18\u001b[0m irrel \u001b[38;5;241m=\u001b[39m module(irrelevant) \u001b[38;5;241m-\u001b[39m bias\n",
      "File \u001b[0;32m~/.conda/envs/PyTc/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/PyTc/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/PyTc/lib/python3.12/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (5x320 and 1600x100)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 8), facecolor='white')\n",
    "\n",
    "for x, im_num in tqdm(enumerate(im_nums)):\n",
    "\n",
    "    im_torch, im_orig, label_num = dset.get_im_and_label(im_num, device=device) # this will download the mnist dataset\n",
    "    preds = dset.pred_ims(model, im_orig, device=device).flatten()\n",
    "    ind = np.argpartition(preds, -8)[-8:] # top-scoring indexes\n",
    "    ind = ind[np.argsort(preds[ind])][::-1] # sort the indexes\n",
    "    scores = get_diff_scores(im_torch, im_orig, label_num, model, preds, sweep_dim)\n",
    "\n",
    "    # plot raw image\n",
    "    num_rows = len(im_nums)\n",
    "    num_cols = len(scores) + 1\n",
    "    plt.subplot(num_rows, num_cols, 1 + x * num_cols)\n",
    "    plt.imshow(im_orig, cmap='gray')\n",
    "    plt.gca().xaxis.set_visible(False)\n",
    "    plt.yticks([])\n",
    "    if x == 0:\n",
    "        plt.title('Image', fontsize=16)\n",
    "\n",
    "\n",
    "    # plot scores\n",
    "    vmax = max([np.max(scores[i]) for i in range(len(scores))])\n",
    "    vmin = min([np.min(scores[i]) for i in range(len(scores))])\n",
    "    vabs = max(abs(vmax), abs(vmin))\n",
    "    for i, tit in enumerate(['CD', 'Occlusion', 'Build-Up', 'IG']):\n",
    "        plt.subplot(num_rows, num_cols, 2 + i + x * num_cols)\n",
    "        if i == 0:\n",
    "            plt.ylabel('pred: ' + str(ind[0]) + '...', fontsize=15)\n",
    "        if x == 0:\n",
    "            plt.title(tit, fontsize=16)\n",
    "        p = viz.visualize_preds(scores[i], num=label_num, cbar=False) #axis_off=False,  vabs=vabs)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "#     divider = make_axes_locatable(plt.gca())\n",
    "#     cax = divider.append_axes(\"right\", size=\"2%\", pad=0.05)\n",
    "#     plt.colorbar(p, cax=cax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
